{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1db35e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('words')\n",
    "plt.style.use('ggplot')\n",
    "nltk.download('cmudict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ebde3",
   "metadata": {},
   "source": [
    "# Extracting all the necessary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951bcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extracting stopwords list\n",
    "def getstopwords(filename):\n",
    "    \n",
    "    with open(filename, 'r') as stopwords_file:\n",
    "        stop_words = stopwords_file.read().splitlines()\n",
    "    return stop_words\n",
    "\n",
    "stop_words1 = getstopwords(\"StopWords_Generic.txt\")\n",
    "stop_words2 = getstopwords(\"StopWords_GenericLong.txt\")\n",
    "stop_words3 = getstopwords(\"StopWords_Auditor.txt\")\n",
    "stop_words4 = getstopwords(\"StopWords_Currencies.txt\")\n",
    "stop_words5 = getstopwords(\"StopWords_DatesandNumbers.txt\")\n",
    "stop_words6 = getstopwords(\"StopWords_Geographic.txt\")\n",
    "stop_words7 = getstopwords(\"StopWords_Names.txt\")\n",
    "\n",
    "stop_wordss = stop_words1+stop_words2+stop_words3+stop_words4+stop_words5+stop_words6+stop_words7\n",
    "\n",
    "\n",
    "#extracting positive and negetive words list:\n",
    "\n",
    "with open(\"positive-words.txt\" , 'r') as file:\n",
    "    pos_words = file.read().splitlines()\n",
    "with open(\"negative-words.txt\" , 'r') as file:\n",
    "    neg_words = file.read().splitlines()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ef408",
   "metadata": {},
   "source": [
    "# All the required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd247b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting content of article\n",
    "\n",
    "def extractcontent(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title = soup.title.text.strip()\n",
    "    \n",
    "    contentt = ''\n",
    "    article_body = soup.find('div', class_='td-post-content')\n",
    "\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        contentt = '\\n'.join([p.text.strip() for p in paragraphs])\n",
    "    strcont = title+contentt   \n",
    "\n",
    "    return strcont\n",
    "\n",
    "#function for removing stopwords\n",
    "\n",
    "def remove_stopwords(input_text, stopwords_list):\n",
    "    stopwords_list1 = list(map(str.lower, stopwords_list))\n",
    "    words = input_text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords_list1]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#function that will perform sentimental analysis\n",
    "\n",
    "def sentiment_analysis(text, positive_words, negative_words):\n",
    "    words = text.lower().split()\n",
    "\n",
    "    positive_count = sum(word in positive_words for word in words)\n",
    "    negative_count = sum(word in negative_words for word in words)\n",
    "\n",
    "    total_words = len(words)\n",
    "    positive_score = positive_count\n",
    "    negative_score = negative_count\n",
    "    polarity_score = (positive_score - negative_score)/((positive_score + negative_score)+0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / ((total_words)+0.000001)\n",
    "\n",
    "    return positive_score , negative_score, polarity_score ,subjectivity_score\n",
    "\n",
    "\n",
    "#functions that will perform readability analysis\n",
    "       \n",
    "    \n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]) if word.lower() in d else 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    average_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    percentage_complex_words = (len(complex_words) / len(words)) * 100 if len(words) > 0 else 0\n",
    "    fogindex = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    avg_words = average_sentence_length\n",
    "\n",
    "    return average_sentence_length, percentage_complex_words , fogindex, avg_words , len(complex_words)\n",
    "\n",
    "\n",
    "#function that will count total words\n",
    "\n",
    "def count_totalwords(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    total_words = len(words)\n",
    "\n",
    "    return total_words\n",
    "\n",
    "\n",
    "#functions that will count average syllables\n",
    "\n",
    "def count_syllables2(word):\n",
    "    cleaned_word = re.sub(r'(es|ed)$', '', word, flags=re.IGNORECASE)\n",
    "    vowels = 'aeiouy'\n",
    "    return sum(1 for char in cleaned_word.lower() if char in vowels)\n",
    "\n",
    "def calculate_average_syllables(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    syllable_counts = [count_syllables2(word) for word in words]\n",
    "\n",
    "    total_syllables = sum(syllable_counts)\n",
    "    total_words = len(words)\n",
    "    average_syllables_per_word = total_syllables / total_words if total_words > 0 else 0\n",
    "\n",
    "    return average_syllables_per_word\n",
    "\n",
    "\n",
    "#function that will count no. of personal pronouns\n",
    "\n",
    "def count_personal_pronouns(text):\n",
    "    target_words = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in target_words) + r')\\b'\n",
    "    exclude_pattern = r'\\bUS\\b'\n",
    "\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    excluded_matches = re.findall(exclude_pattern, text)\n",
    "\n",
    "    word_count = len(matches) - len(excluded_matches)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "#function that will count average word length\n",
    "\n",
    "def calculate_average_word_length(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return average_word_length\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0edc43",
   "metadata": {},
   "source": [
    "# Function that will perform text analysis on a URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0869a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysisfunction(url1 , stop_wordsss , pos_wordss , neg_wordss):\n",
    "    \n",
    "    content = extractcontent(url1)\n",
    "    \n",
    "    filtered_content = remove_stopwords(content,stop_wordsss)\n",
    "    \n",
    "    results1 = list(sentiment_analysis(filtered_content, pos_wordss, neg_wordss))\n",
    "    \n",
    "    results2 = list(calculate_metrics(content))\n",
    "    \n",
    "    results3 = count_totalwords(content)\n",
    "    \n",
    "    results4 = calculate_average_syllables(content)\n",
    "    \n",
    "    results5 = count_personal_pronouns(content)\n",
    "    \n",
    "    results6 = calculate_average_word_length(content)\n",
    "    \n",
    "    combine_results = results1+results2\n",
    "    combine_results.append(results3)\n",
    "    combine_results.append(results4)\n",
    "    combine_results.append(results5)\n",
    "    combine_results.append(results6)\n",
    "    \n",
    "    return combine_results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b98a85",
   "metadata": {},
   "source": [
    "# Extracting excel file that contains URL_id and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bac919d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             NaN             NaN             NaN                 NaN   \n",
       "1             NaN             NaN             NaN                 NaN   \n",
       "2             NaN             NaN             NaN                 NaN   \n",
       "3             NaN             NaN             NaN                 NaN   \n",
       "4             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  NaN                          NaN        NaN   \n",
       "1                  NaN                          NaN        NaN   \n",
       "2                  NaN                          NaN        NaN   \n",
       "3                  NaN                          NaN        NaN   \n",
       "4                  NaN                          NaN        NaN   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                               NaN                 NaN         NaN   \n",
       "1                               NaN                 NaN         NaN   \n",
       "2                               NaN                 NaN         NaN   \n",
       "3                               NaN                 NaN         NaN   \n",
       "4                               NaN                 NaN         NaN   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                NaN                NaN              NaN  \n",
       "1                NaN                NaN              NaN  \n",
       "2                NaN                NaN              NaN  \n",
       "3                NaN                NaN              NaN  \n",
       "4                NaN                NaN              NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('Output.xlsx')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5064809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             NaN             NaN             NaN                 NaN   \n",
       "1             NaN             NaN             NaN                 NaN   \n",
       "2             NaN             NaN             NaN                 NaN   \n",
       "3             NaN             NaN             NaN                 NaN   \n",
       "4             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  NaN                          NaN        NaN   \n",
       "1                  NaN                          NaN        NaN   \n",
       "2                  NaN                          NaN        NaN   \n",
       "3                  NaN                          NaN        NaN   \n",
       "4                  NaN                          NaN        NaN   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                               NaN                 NaN         NaN   \n",
       "1                               NaN                 NaN         NaN   \n",
       "2                               NaN                 NaN         NaN   \n",
       "3                               NaN                 NaN         NaN   \n",
       "4                               NaN                 NaN         NaN   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                NaN                NaN              NaN  \n",
       "1                NaN                NaN              NaN  \n",
       "2                NaN                NaN              NaN  \n",
       "3                NaN                NaN              NaN  \n",
       "4                NaN                NaN              NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79646e2",
   "metadata": {},
   "source": [
    "# Performing data analysis iteratively on every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92f9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    \n",
    "    url = row['URL']\n",
    "\n",
    "    \n",
    "    results = list(analysisfunction(url ,stop_wordss , pos_words , neg_words ))\n",
    "\n",
    " \n",
    "    data.at[index, 'POSITIVE SCORE'] = results[0]\n",
    "    data.at[index, 'NEGATIVE SCORE'] = results[1]\n",
    "    data.at[index, 'POLARITY SCORE'] = results[2]\n",
    "    data.at[index, 'SUBJECTIVITY SCORE'] = results[3]\n",
    "    data.at[index, 'AVG SENTENCE LENGTH'] = results[4]\n",
    "    data.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = results[5]\n",
    "    data.at[index, 'FOG INDEX'] = results[6]\n",
    "    data.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = results[7]\n",
    "    data.at[index, 'COMPLEX WORD COUNT'] = results[8]\n",
    "    data.at[index, 'WORD COUNT'] = results[9]\n",
    "    data.at[index, 'SYLLABLE PER WORD'] = results[10]\n",
    "    data.at[index, 'PERSONAL PRONOUNS'] = results[11]\n",
    "    data.at[index, 'AVG WORD LENGTH'] = results[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d9480",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "134bf854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>15.880000</td>\n",
       "      <td>10.831234</td>\n",
       "      <td>10.684494</td>\n",
       "      <td>15.880000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.637280</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.678771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>21.181818</td>\n",
       "      <td>20.294298</td>\n",
       "      <td>16.590446</td>\n",
       "      <td>21.181818</td>\n",
       "      <td>331.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>1.852238</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.631095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.082258</td>\n",
       "      <td>21.803571</td>\n",
       "      <td>27.027027</td>\n",
       "      <td>19.532239</td>\n",
       "      <td>21.803571</td>\n",
       "      <td>330.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>2.050778</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.287723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>23.784314</td>\n",
       "      <td>22.588623</td>\n",
       "      <td>18.549175</td>\n",
       "      <td>23.784314</td>\n",
       "      <td>274.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>2.003298</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.125356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.063768</td>\n",
       "      <td>19.641026</td>\n",
       "      <td>18.407311</td>\n",
       "      <td>15.219335</td>\n",
       "      <td>19.641026</td>\n",
       "      <td>141.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1.877285</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.672012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             2.0             1.0        0.333333            0.018987   \n",
       "1            40.0            23.0        0.269841            0.089872   \n",
       "2            32.0            19.0        0.254902            0.082258   \n",
       "3            29.0            61.0       -0.355556            0.150000   \n",
       "4            15.0             7.0        0.363636            0.063768   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            15.880000                    10.831234  10.684494   \n",
       "1            21.181818                    20.294298  16.590446   \n",
       "2            21.803571                    27.027027  19.532239   \n",
       "3            23.784314                    22.588623  18.549175   \n",
       "4            19.641026                    18.407311  15.219335   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                         15.880000                43.0       192.0   \n",
       "1                         21.181818               331.0       865.0   \n",
       "2                         21.803571               330.0       677.0   \n",
       "3                         23.784314               274.0       677.0   \n",
       "4                         19.641026               141.0       418.0   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           1.637280                3.0         4.678771  \n",
       "1           1.852238                4.0         5.631095  \n",
       "2           2.050778               13.0         6.287723  \n",
       "3           2.003298                5.0         6.125356  \n",
       "4           1.877285                6.0         5.672012  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e509404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 15 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   URL_ID                            100 non-null    object \n",
      " 1   URL                               100 non-null    object \n",
      " 2   POSITIVE SCORE                    100 non-null    float64\n",
      " 3   NEGATIVE SCORE                    100 non-null    float64\n",
      " 4   POLARITY SCORE                    100 non-null    float64\n",
      " 5   SUBJECTIVITY SCORE                100 non-null    float64\n",
      " 6   AVG SENTENCE LENGTH               100 non-null    float64\n",
      " 7   PERCENTAGE OF COMPLEX WORDS       100 non-null    float64\n",
      " 8   FOG INDEX                         100 non-null    float64\n",
      " 9   AVG NUMBER OF WORDS PER SENTENCE  100 non-null    float64\n",
      " 10  COMPLEX WORD COUNT                100 non-null    float64\n",
      " 11  WORD COUNT                        100 non-null    float64\n",
      " 12  SYLLABLE PER WORD                 100 non-null    float64\n",
      " 13  PERSONAL PRONOUNS                 100 non-null    float64\n",
      " 14  AVG WORD LENGTH                   100 non-null    float64\n",
      "dtypes: float64(13), object(2)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af7e5413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.540000</td>\n",
       "      <td>18.690000</td>\n",
       "      <td>0.197885</td>\n",
       "      <td>0.083823</td>\n",
       "      <td>25.297757</td>\n",
       "      <td>15.859979</td>\n",
       "      <td>16.463094</td>\n",
       "      <td>25.297757</td>\n",
       "      <td>193.220000</td>\n",
       "      <td>608.230000</td>\n",
       "      <td>1.761719</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>5.256267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.113499</td>\n",
       "      <td>15.458445</td>\n",
       "      <td>0.433655</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>19.550462</td>\n",
       "      <td>4.525274</td>\n",
       "      <td>8.099281</td>\n",
       "      <td>19.550462</td>\n",
       "      <td>110.556954</td>\n",
       "      <td>340.303722</td>\n",
       "      <td>0.125629</td>\n",
       "      <td>6.785308</td>\n",
       "      <td>0.382451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.517727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.643519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-0.112403</td>\n",
       "      <td>0.063814</td>\n",
       "      <td>19.898160</td>\n",
       "      <td>12.965052</td>\n",
       "      <td>13.676638</td>\n",
       "      <td>19.898160</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>390.250000</td>\n",
       "      <td>1.678595</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.948705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.252451</td>\n",
       "      <td>0.080951</td>\n",
       "      <td>23.226537</td>\n",
       "      <td>15.554705</td>\n",
       "      <td>15.905675</td>\n",
       "      <td>23.226537</td>\n",
       "      <td>192.500000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>1.746004</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.219935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.250000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.104512</td>\n",
       "      <td>26.734698</td>\n",
       "      <td>18.850403</td>\n",
       "      <td>17.985914</td>\n",
       "      <td>26.734698</td>\n",
       "      <td>265.500000</td>\n",
       "      <td>810.750000</td>\n",
       "      <td>1.831207</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.504726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>210.222222</td>\n",
       "      <td>27.865169</td>\n",
       "      <td>89.670284</td>\n",
       "      <td>210.222222</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>2275.000000</td>\n",
       "      <td>2.101463</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.443726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "count      100.000000      100.000000      100.000000          100.000000   \n",
       "mean        24.540000       18.690000        0.197885            0.083823   \n",
       "std         16.113499       15.458445        0.433655            0.035709   \n",
       "min          0.000000        0.000000       -1.000000            0.000000   \n",
       "25%         14.000000        6.000000       -0.112403            0.063814   \n",
       "50%         23.000000       16.500000        0.252451            0.080951   \n",
       "75%         32.250000       27.000000        0.480124            0.104512   \n",
       "max         73.000000       62.000000        1.000000            0.189189   \n",
       "\n",
       "       AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS   FOG INDEX  \\\n",
       "count           100.000000                   100.000000  100.000000   \n",
       "mean             25.297757                    15.859979   16.463094   \n",
       "std              19.550462                     4.525274    8.099281   \n",
       "min               6.000000                     0.000000    2.400000   \n",
       "25%              19.898160                    12.965052   13.676638   \n",
       "50%              23.226537                    15.554705   15.905675   \n",
       "75%              26.734698                    18.850403   17.985914   \n",
       "max             210.222222                    27.865169   89.670284   \n",
       "\n",
       "       AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT   WORD COUNT  \\\n",
       "count                        100.000000          100.000000   100.000000   \n",
       "mean                          25.297757          193.220000   608.230000   \n",
       "std                           19.550462          110.556954   340.303722   \n",
       "min                            6.000000            0.000000     4.000000   \n",
       "25%                           19.898160          114.500000   390.250000   \n",
       "50%                           23.226537          192.500000   622.000000   \n",
       "75%                           26.734698          265.500000   810.750000   \n",
       "max                          210.222222          512.000000  2275.000000   \n",
       "\n",
       "       SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "count         100.000000         100.000000       100.000000  \n",
       "mean            1.761719           6.200000         5.256267  \n",
       "std             0.125629           6.785308         0.382451  \n",
       "min             1.517727           0.000000         4.643519  \n",
       "25%             1.678595           2.000000         4.948705  \n",
       "50%             1.746004           4.000000         5.219935  \n",
       "75%             1.831207           8.000000         5.504726  \n",
       "max             2.101463          37.000000         6.443726  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb55d19",
   "metadata": {},
   "source": [
    "# Converting the DataFrame to final Output excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea2087dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bc43000",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa.to_excel('Final Output Data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279845ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
